{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09fed25",
   "metadata": {
    "papermill": {
     "duration": 0.005114,
     "end_time": "2022-08-06T11:49:46.607978",
     "exception": false,
     "start_time": "2022-08-06T11:49:46.602864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Multi-armed bandit\n",
    "## Chapter1. Two characters : Exploration and Exploitation\n",
    "### The Scientist and the Businessman\n",
    "\n",
    "로고를 다른 색으로 바꿀까 말까?\n",
    "- sicentist : 바꺼! A/B test - 동전 합쳐서 \n",
    "- businessman : 너가 마음에 들면 바꿔도 되지만, 돈드니까\n",
    " 위가 나오면 old version, 아래가 나오면 new version 줘서 뭐가 더 반응 좋은지 봐!\n",
    " -> 더 다양하게 해서 A, B, C, D, E test 해봐!\n",
    " \n",
    " \n",
    " 즉 There are Exploitation and Exploration Dillema있음.\n",
    " 새로운 option을 도전하는 Exploreㅁ만 ㅁ낳이 하면, 좋은 것을 최고로 많이 해서 보상을 많이 얻을 수 없게 됨.\n",
    " (여기에서 색깔 원하는거 다 test하고 다 물어보면 돈이 너무 많이 들고 비효율적)\n",
    " \n",
    " 그렇다고 그냥 내가 아는 것 중에서 제일 좋다고 (보상이 높다고) 생각되는 것들만 게속 이용하는 \n",
    " Exploit만 하겠다는 것도\n",
    " 지금 내가 과거 데이터를 기반으로 해서 Best라고 믿고 있었지만, 실은 더 좋은 색깔있을 수도 있는거니까\n",
    " \n",
    " -> 이 두가지 모두를 동시에 많이 할 수 는 없기 때문에,\n",
    " 우리는 이를  \"The Explore-Exploit Dilemma\"라고 함.\n",
    " 어떤 것을 얼마만큼 더 많이 하면 좋을지는 상황에 따라 달라지기 때문에,\n",
    " 단순히 A/B test하는게 아니라\n",
    " \n",
    " \n",
    " ### A/B 테스트로 충분하지 않고 Bandit Algorithm이 필요한 이유\n",
    " A/B 테스트는 둘 중 뭐가 더 효과적인지 알아낼 때까지 실험을 지속하고,\n",
    " 뭐가 더 좋다! 라고 결론이 나면 \n",
    " 그것을 적용하고 이제, 더 좋지 않다고 결론 난 것에 대해서는 버려버려! 더 생각하지 않음.\n",
    " \n",
    " A/B test는  \n",
    " - ' IT jumps discretey from exploration into exploitation',\n",
    " - ' During the purely exploratory phase, it wastes resources exploring inferior options in order to gather as much data as possible'\n",
    " 의 두가지 문제점\n",
    " \n",
    " (즉 explore 짧게 한걸로 평생 exploit한다는 것인데, 이는 **세상이 변하지 않는다**는 가정한것,\n",
    " 하지만, 세상은 늘 변함.)\n",
    "\n",
    "-> Bandit Algorithm은 이 두 문제를 다룸.\n",
    "exploration 하는 비율을 smoothly하게 줄이고, explore할 때 더 좋은 optiopn에 대해서 집중함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c1328",
   "metadata": {
    "papermill": {
     "duration": 0.003983,
     "end_time": "2022-08-06T11:49:46.616158",
     "exception": false,
     "start_time": "2022-08-06T11:49:46.612175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Chapter3. The epsilon-Greedy Algorithm\n",
    "### Introduction.\n",
    "컴퓨터 사이언스 : Greedy Algorithm _ 무조건 현재에서 최상의 선택을 하는 option을 구하자!\n",
    "\n",
    "\n",
    "하지만,\n",
    "더 좋은 option이 있을지도 모르니까\n",
    "$\\epsilon$ 만큼 확률 ( algorithm이 exploiting하는 대신 exploration하는 확률)로는 explore하자!\n",
    "\n",
    "방법은 단순!\n",
    "동전을 던져서 (Bernoulli Distribution) \n",
    "\n",
    "- 앞면이 나오면 (앞 면이 나올 확률은 $\\epsilon$), 랜덤하게 explore,\n",
    "- 뒷면이 나오면 (뒷 면이 나올 확률은 $1-\\epsilon$), 과거의 데이터 이용해서 성공 확률이 가장 높은 것을 선택\n",
    "\n",
    "### Jargon\n",
    "#### arm은 선택할 수 있는 option을 지칭!\n",
    "we assume that we have a fixed set of N different options and that we can enumerate them.\n",
    "\n",
    "option1, $\\cdots$, option N\n",
    "\n",
    "#### reward는 성공을 measure하는 값\n",
    "reward is  simply a measure of success\n",
    "\n",
    "예를 들어 고객들이 클릭을 하는지 안하는지,\n",
    "등록을 하는지 안하는지 여부로 정의 가능\n",
    "\n",
    "reward는 반드시 \n",
    "- quantitative 해서 수학적으로 track가능한 값이어야 하고,\n",
    "- 작은 값보다 reward가 큰 것이 better\n",
    "\n",
    "\n",
    "### Bandt Problem\n",
    "- a bandit which has a set of N arms =  N개의 옵션이 있는 문제\n",
    "- 하나의 옵션(pull any give arm)을 선택하면, 그에 따른 보상(reward)가 있다. (하지만 우리는 이 reward의 분포를 모름. riscky)\n",
    "- we do NOT start off kwing what the reward rates are for any of the arms. -> 이 값을 유추하기 위해서 arm들 당겨보는 것!\n",
    "\n",
    "우리가 하고 싶은건 가장 큰 reward 평균 값을 가지는 arm을 알아내고 싶음 \n",
    "But, 문제는 reward에 대한 정보가 너무 부족\n",
    "1) partial feedback문제 : 당겨본 arm에 대한 정보만 가지고 있지, 그 arm댕기느라 선택하지 못한 다른 arm에 대한 정보는 없음\n",
    "\n",
    "2) falling bhind문제 : 매번 best option을 뽑지 못하니까 lose reward -> 우리는 더 나은 arm 땡길수도 있었는데...\n",
    "\n",
    "\n",
    "### 즉 MAP (Multi Armed Bandit) 이란,\n",
    "- 정확한 승률을 알지 못하는 여러대의 slot machines을 가지고 도박을 할 때, 가장 많은 돈을 따고 싶지만 각 slot 마다 Reward확률 이다르고 어떤 slot의 reward가 가장 큰지 알지 못함.\n",
    "- 이런 상황에서 \" 어떤 순서로 어떤 slot을 얼마만큼 당겨야 가장 많은 reward (cumulative reward) 를 얻을 것인가\"의 문제!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2e30d",
   "metadata": {
    "papermill": {
     "duration": 0.003963,
     "end_time": "2022-08-06T11:49:46.624237",
     "exception": false,
     "start_time": "2022-08-06T11:49:46.620274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Greedy Algorithm\n",
    "- 모든 slot machine 한번씩 play하고 reward가장 좋은 쪽을 exploit\n",
    "- 하지만 충분한 탐색이 이루어지지 않는다는 점\n",
    "\n",
    "## Epilon Greedy Algorithm\n",
    "- 1 - $\\epsilon$ 확률로는 exploitation\n",
    "-$\\epsilon$만큼은 exploration\n",
    "\n",
    "-> epsilon 모수가 eploration&exploitation trade-off를 조절 : 만약 이 값이 크면 explore 많이 하는것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73277c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-06T11:49:46.634584Z",
     "iopub.status.busy": "2022-08-06T11:49:46.633628Z",
     "iopub.status.idle": "2022-08-06T11:49:51.409134Z",
     "shell.execute_reply": "2022-08-06T11:49:51.408171Z"
    },
    "papermill": {
     "duration": 4.783624,
     "end_time": "2022-08-06T11:49:51.411817",
     "exception": false,
     "start_time": "2022-08-06T11:49:46.628193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027b4cf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-06T11:49:51.421933Z",
     "iopub.status.busy": "2022-08-06T11:49:51.421191Z",
     "iopub.status.idle": "2022-08-06T11:49:51.431841Z",
     "shell.execute_reply": "2022-08-06T11:49:51.431188Z"
    },
    "papermill": {
     "duration": 0.01758,
     "end_time": "2022-08-06T11:49:51.433717",
     "exception": false,
     "start_time": "2022-08-06T11:49:51.416137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 설계\n",
    "# epsilon : a floating point number that tells us the frequency with which we should explore one of the available arms.\n",
    "# set epsilon = 0.1\n",
    "\n",
    "# counts : a vector of inteers of length N ( 각 arm을 몇번 당겼는지 담긴 벡터. 길이는 arm만큼이니까 N)\n",
    "# 이 예제에서는 Arm !, Arm 2 2개가 있고 각각 2번씩만 당겼으니까\n",
    "# counts = [2,2]\n",
    "\n",
    "# values : a verctor of floating point numbers \n",
    "#               that defines the average amount of reward we've gotten when playing N arms available to us.\n",
    "# 즉 보상의 평균, Arm 1 이 2번 당겼을 때 각각 1,0의 output을 내고, Arm 2rk 0,0의 output을 내면,\n",
    "# values = [0.5,0.0]\n",
    "import random\n",
    "\n",
    "def ind_max(x):\n",
    "    m = max(x)\n",
    "    return x.index(m)\n",
    "\n",
    "#Epsilon Greedy 클래스 정의\n",
    "class EpsilonGreedy():\n",
    "    def __init__(self,epsilon,counts,values):\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        return\n",
    "    \n",
    "    # provide explicit initialization method \n",
    "    #_counts, values변수를 알고리즘 돌아가기전 적절한 blank slate states로 리셋하는 함수\n",
    "    def initialize(self,n_arms):\n",
    "        self.counts = [0 for col in range(n_arms)]\n",
    "        self.values = [0.0 for col in range(n_arms)]\n",
    "        return\n",
    "\n",
    "\n",
    "    # select_arm 함수\n",
    "    # arm을 당기는 선택을 할 때 부르는 함수_ 당겨야 할 다음 arm의 index번호를 호출\n",
    "    # 아무 argument 없이 부르고, \n",
    "    # return : index of the next arm to pull\n",
    "\n",
    "\n",
    "    def select_arm(self):\n",
    "        if random.random() > self.epsilon: #클래스에서 정의된 episilon값보다 큰 값이 나오면,\n",
    "            return ind_max(self.values) # 지금 가지고 있는 values(reward의 평균)이 가장 큰 곳의 index뽑아 return\n",
    "        else:\n",
    "            return random.randrange(len(self.values)) #explore 탐색_ select the arm completely random\n",
    "        \n",
    "\n",
    "    # upate 함수\n",
    "    # arm을 당기고 나면 reward를 얻는데, 이를 업데이터 해줘야해.\n",
    "    # input인자로는 alogirithm object, 방금 당긴 arm의 index, 방금 당긴 arm에 대한 reward\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] = self.counts[chosen_arm] + 1 #그 arm을 당긴 횟수를 저장하는 count값을 하나 올려줌\n",
    "        n = self.counts[chosen_arm]\n",
    "    \n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ((n-1)/float(n))*value + (1/float(n)) * reward #그 arm을 당긴 보상의 평균을 구하기\n",
    "        self.values[chosen_arm] = new_value #그 arm을 당긴 보상의 평균을 구한값 그 인덱스에 맞게 넣어주기\n",
    "        return\n",
    "\n",
    "    #추후에는 평균을 구하는게 아니라, average onine을 계산하기 위해서 alternative weighting scheme을 이용할 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee6019",
   "metadata": {
    "papermill": {
     "duration": 0.00372,
     "end_time": "2022-08-06T11:49:51.441279",
     "exception": false,
     "start_time": "2022-08-06T11:49:51.437559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### epsilon 결정하기_epsilon greedy의 한계점\n",
    "- $\\epsilon$ = 1.0이라면, 늘 완전히 random 하게 option을 선택. -> waste source acquiring data aoubt bad options\n",
    "- $\\epsilon$ = 0.0이라면, 가장 average reward높게 나온 option을 계속 당김 -> 새로운 것을 탐색할 수 없음.\n",
    "\n",
    "하지만 epsilon greedy algorithm은 \n",
    "- 최적 slot machine 찾은 후에도 계속해서 epsilon만큼은 랜덤하게 막 탐색한다는 한계점이 있다.\n",
    "- 입실론 만큼 확률로 sub optimal 한 나머지 slot machine을 무작위로 뽑아서, 전체 슬롯 머신 중 탐색하지 못하거나, 혹은 탐색을 덜해서 정보를 얻지 못한 슬롯 머신이 생길 가능성이큼.\n",
    "\n",
    "이를 보완하기 위해서 나온 알고리즘이 softmax!\n",
    "\n",
    "그 전에, 먼저 **Monte Carlo simulation**으로 random number generator사용해서 simulation해.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ee707",
   "metadata": {
    "papermill": {
     "duration": 0.003465,
     "end_time": "2022-08-06T11:49:51.448456",
     "exception": false,
     "start_time": "2022-08-06T11:49:51.444991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CHAPTER4. Debugging Bandit Algorithms\n",
    "\n",
    "### Monte Carlo Simulations ARe Like Unit Tests for Bandit Algorithms\n",
    "\n",
    "bandit 알고리즘은 T시점에 select_arm()하면 그에 대한 reward가 있어서 update가 되야함.\n",
    "\n",
    "따라서 그냥 평균을 업데이트 하는 방법 대신, **Monte Carlo simulation 방법**을 택함.\n",
    "\n",
    "-> 2개이의 코드 짤 것임\n",
    "\n",
    "이 두 개의 코드가 함께 실제 상황에서 이 알고리즘이 어떻게 구현되는지 보여줌\n",
    "- bandit algorithm\n",
    "- a simulation of the bandit's arms that the algorithm as to select between\n",
    "\n",
    "simulation은 random number에 의해 돌아가므로 noise가 있음.\n",
    "-> 따라서 여러번 돌려서 develop an intuition for its behavior in different settins.\n",
    "\n",
    "\n",
    "### Simulating the Arms of a Bandit Problem\n",
    "hypothetical arm이 필요함\n",
    "보상이 click throughs ro user signups같은 0/1 문제일 때\n",
    "\n",
    "-> 우리는 이런 상황을 **\"a Bernoulli Arm\"** 이라고 부름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f1db75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-06T11:49:51.457464Z",
     "iopub.status.busy": "2022-08-06T11:49:51.456951Z",
     "iopub.status.idle": "2022-08-06T11:49:51.461550Z",
     "shell.execute_reply": "2022-08-06T11:49:51.460918Z"
    },
    "papermill": {
     "duration": 0.011071,
     "end_time": "2022-08-06T11:49:51.463256",
     "exception": false,
     "start_time": "2022-08-06T11:49:51.452185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bernoulli ARm\n",
    "class BernoulliArm():\n",
    "    def __init__(self, p): # p는 arm으로 부터 1의 reward를 얻을 확률을 의미.\n",
    "        self.p = p\n",
    "  \n",
    "    def draw(self):\n",
    "        if random.random() > self.p:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab281434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-06T11:49:51.472160Z",
     "iopub.status.busy": "2022-08-06T11:49:51.471632Z",
     "iopub.status.idle": "2022-08-06T11:49:51.481038Z",
     "shell.execute_reply": "2022-08-06T11:49:51.480371Z"
    },
    "papermill": {
     "duration": 0.015978,
     "end_time": "2022-08-06T11:49:51.482919",
     "exception": false,
     "start_time": "2022-08-06T11:49:51.466941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#core infrastructure and testing framework\n",
    "# initialize the bandit algorithm's setting from scratch \n",
    "#                             -> no prior knowledge about which arm is best.\n",
    "\n",
    "def test_algorithm(algo, arms, num_sims, horizon):\n",
    "    chosen_arms = [0.0 for i in range(num_sims * horizon)]\n",
    "    rewards = [0.0 for i in range(num_sims * horizon)]\n",
    "    cumulative_rewards = [0.0 for i in range(num_sims * horizon)]\n",
    "    sim_nums = [0.0 for i in range(num_sims * horizon)]\n",
    "    times = [0.0 for i in range(num_sims * horizon)]\n",
    "  \n",
    "    for sim in range(num_sims):\n",
    "        sim = sim + 1\n",
    "        algo.initialize(len(arms))\n",
    "    \n",
    "    for t in range(horizon): # 이제 pull arm\n",
    "        t = t + 1\n",
    "        index = (sim - 1) * horizon + t - 1\n",
    "        sim_nums[index] = sim\n",
    "        times[index] = t\n",
    "      \n",
    "        # select the arm_ 당겨야 할 다음 arm의 index번호를 호출\n",
    "        chosen_arm = algo.select_arm() \n",
    "        chosen_arms[index] = chosen_arm\n",
    "      \n",
    "        # 당겼을 때 reward 여기선 0 or 1\n",
    "        reward = arms[chosen_arms[index]].draw()\n",
    "        rewards[index] = reward\n",
    "      \n",
    "        # update cumulative reward _ 처음 당길 때 제외하고는 그전 값에 이번 reward 더해가면서\n",
    "        if t == 1:\n",
    "            cumulative_rewards[index] = reward\n",
    "        else:\n",
    "            cumulative_rewards[index] = cumulative_rewards[index - 1] + reward\n",
    "      \n",
    "        algo.update(chosen_arm, reward)\n",
    "  \n",
    "    return [sim_nums, times, chosen_arms, rewards, cumulative_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5cf10c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-06T11:49:51.492096Z",
     "iopub.status.busy": "2022-08-06T11:49:51.491719Z",
     "iopub.status.idle": "2022-08-06T11:50:13.652677Z",
     "shell.execute_reply": "2022-08-06T11:50:13.651414Z"
    },
    "papermill": {
     "duration": 22.168605,
     "end_time": "2022-08-06T11:50:13.655488",
     "exception": false,
     "start_time": "2022-08-06T11:49:51.486883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best arm is 2\n"
     ]
    }
   ],
   "source": [
    "#execfile(\"core.py\")\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# set up an array of Arm objects\n",
    "means = [0.1, 0.1, 0.1, 0.1, 0.9] # 5개의 arm 생성. 4개는 0.1 reward, 1개는 0.9 reward.\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "f = open(\"D:\\coding\\bandit\\algorithms\\epsilon_greedy\\standard_results.tsv\", \"w\")\n",
    "\n",
    "for epsilon in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    algo = EpsilonGreedy(epsilon, [], [])\n",
    "    algo.initialize(n_arms)\n",
    "    results = test_algorithm(algo, arms, 5000, 250)\n",
    "    for i in range(len(results[0])):\n",
    "        f.write(str(epsilon) + \"\\t\")\n",
    "        f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbc3c2",
   "metadata": {
    "papermill": {
     "duration": 0.00411,
     "end_time": "2022-08-06T11:50:13.664064",
     "exception": false,
     "start_time": "2022-08-06T11:50:13.659954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560ca892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-06T11:50:13.674055Z",
     "iopub.status.busy": "2022-08-06T11:50:13.673623Z",
     "iopub.status.idle": "2022-08-06T11:50:13.683338Z",
     "shell.execute_reply": "2022-08-06T11:50:13.682574Z"
    },
    "papermill": {
     "duration": 0.016908,
     "end_time": "2022-08-06T11:50:13.685161",
     "exception": false,
     "start_time": "2022-08-06T11:50:13.668253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#annealing\n",
    "import random\n",
    "import math\n",
    "\n",
    "def ind_max(x):\n",
    "    m = max(x)\n",
    "    return x.index(m)\n",
    "\n",
    "class AnnealingEpsilonGreedy():\n",
    "    def __init__(self, counts, values):\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        return\n",
    "    \n",
    "    def initialize(self, n_arms):\n",
    "        self.counts = [0 for col in range(n_arms)]\n",
    "        self.values = [0.0 for col in range(n_arms)]\n",
    "        return\n",
    "    \n",
    "    def select_arm(self):\n",
    "        t = sum(self.counts) + 1\n",
    "        epsilon = 1 / math.log(t + 0.0000001)\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            return ind_max(self.values)\n",
    "        else:\n",
    "            return random.randrange(len(self.values))\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] = self.counts[chosen_arm] + 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        \n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward\n",
    "        self.values[chosen_arm] = new_value\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc0a025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-06T11:50:13.695076Z",
     "iopub.status.busy": "2022-08-06T11:50:13.694262Z",
     "iopub.status.idle": "2022-08-06T11:50:17.282430Z",
     "shell.execute_reply": "2022-08-06T11:50:17.281289Z"
    },
    "papermill": {
     "duration": 3.596049,
     "end_time": "2022-08-06T11:50:17.285084",
     "exception": false,
     "start_time": "2022-08-06T11:50:13.689035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best arm is 2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "means = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "my_algo = AnnealingEpsilonGreedy([], [])\n",
    "my_algo.initialize(n_arms)\n",
    "results = test_algorithm(my_algo, arms, 5000, 250)\n",
    "\n",
    "f = open(\"D:\\coding\\bandit\\algorithms\\epsilon_greedy\\annealing_results.tsv\", \"w\")\n",
    "for i in range(len(results[0])):\n",
    "    f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e63527",
   "metadata": {
    "papermill": {
     "duration": 0.004588,
     "end_time": "2022-08-06T11:50:17.293949",
     "exception": false,
     "start_time": "2022-08-06T11:50:17.289361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775920df",
   "metadata": {
    "papermill": {
     "duration": 0.00376,
     "end_time": "2022-08-06T11:50:17.301821",
     "exception": false,
     "start_time": "2022-08-06T11:50:17.298061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e17e6",
   "metadata": {
    "papermill": {
     "duration": 0.004037,
     "end_time": "2022-08-06T11:50:17.309799",
     "exception": false,
     "start_time": "2022-08-06T11:50:17.305762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.408967,
   "end_time": "2022-08-06T11:50:20.311718",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-06T11:49:37.902751",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
