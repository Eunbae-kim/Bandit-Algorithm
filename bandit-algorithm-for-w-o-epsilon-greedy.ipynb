{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46760ddf",
   "metadata": {
    "papermill": {
     "duration": 0.013901,
     "end_time": "2022-08-09T02:42:52.611480",
     "exception": false,
     "start_time": "2022-08-09T02:42:52.597579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Multi-armed bandit\n",
    "## Chapter1. Two characters : Exploration and Exploitation\n",
    "### The Scientist and the Businessman\n",
    "\n",
    "로고를 다른 색으로 바꿀까 말까?\n",
    "- sicentist : 바꺼! A/B test - 동전 합쳐서 \n",
    "- businessman : 너가 마음에 들면 바꿔도 되지만, 돈드니까\n",
    " 위가 나오면 old version, 아래가 나오면 new version 줘서 뭐가 더 반응 좋은지 봐!\n",
    " -> 더 다양하게 해서 A, B, C, D, E test 해봐!\n",
    " \n",
    " \n",
    " 즉 There are Exploitation and Exploration Dillema있음.\n",
    " 새로운 option을 도전하는 Exploreㅁ만 ㅁ낳이 하면, 좋은 것을 최고로 많이 해서 보상을 많이 얻을 수 없게 됨.\n",
    " (여기에서 색깔 원하는거 다 test하고 다 물어보면 돈이 너무 많이 들고 비효율적)\n",
    " \n",
    " 그렇다고 그냥 내가 아는 것 중에서 제일 좋다고 (보상이 높다고) 생각되는 것들만 게속 이용하는 \n",
    " Exploit만 하겠다는 것도\n",
    " 지금 내가 과거 데이터를 기반으로 해서 Best라고 믿고 있었지만, 실은 더 좋은 색깔있을 수도 있는거니까\n",
    " \n",
    " -> 이 두가지 모두를 동시에 많이 할 수 는 없기 때문에,\n",
    " 우리는 이를  \"The Explore-Exploit Dilemma\"라고 함.\n",
    " 어떤 것을 얼마만큼 더 많이 하면 좋을지는 상황에 따라 달라지기 때문에,\n",
    " 단순히 A/B test하는게 아니라\n",
    " \n",
    " \n",
    " ### A/B 테스트로 충분하지 않고 Bandit Algorithm이 필요한 이유\n",
    " A/B 테스트는 둘 중 뭐가 더 효과적인지 알아낼 때까지 실험을 지속하고,\n",
    " 뭐가 더 좋다! 라고 결론이 나면 \n",
    " 그것을 적용하고 이제, 더 좋지 않다고 결론 난 것에 대해서는 버려버려! 더 생각하지 않음.\n",
    " \n",
    " A/B test는  \n",
    " - ' IT jumps discretey from exploration into exploitation',\n",
    " - ' During the purely exploratory phase, it wastes resources exploring inferior options in order to gather as much data as possible'\n",
    " 의 두가지 문제점\n",
    " \n",
    " (즉 explore 짧게 한걸로 평생 exploit한다는 것인데, 이는 **세상이 변하지 않는다**는 가정한것,\n",
    " 하지만, 세상은 늘 변함.)\n",
    "\n",
    "-> Bandit Algorithm은 이 두 문제를 다룸.\n",
    "exploration 하는 비율을 smoothly하게 줄이고, explore할 때 더 좋은 optiopn에 대해서 집중함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0eceeb",
   "metadata": {
    "papermill": {
     "duration": 0.011975,
     "end_time": "2022-08-09T02:42:52.636029",
     "exception": false,
     "start_time": "2022-08-09T02:42:52.624054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Chapter3. The epsilon-Greedy Algorithm\n",
    "### Introduction.\n",
    "컴퓨터 사이언스 : Greedy Algorithm _ 무조건 현재에서 최상의 선택을 하는 option을 구하자!\n",
    "\n",
    "\n",
    "하지만,\n",
    "더 좋은 option이 있을지도 모르니까\n",
    "$\\epsilon$ 만큼 확률 ( algorithm이 exploiting하는 대신 exploration하는 확률)로는 explore하자!\n",
    "\n",
    "방법은 단순!\n",
    "동전을 던져서 (Bernoulli Distribution) \n",
    "\n",
    "- 앞면이 나오면 (앞 면이 나올 확률은 $\\epsilon$), 랜덤하게 explore,\n",
    "- 뒷면이 나오면 (뒷 면이 나올 확률은 $1-\\epsilon$), 과거의 데이터 이용해서 성공 확률이 가장 높은 것을 선택\n",
    "\n",
    "### Jargon\n",
    "#### arm은 선택할 수 있는 option을 지칭!\n",
    "we assume that we have a fixed set of N different options and that we can enumerate them.\n",
    "\n",
    "option1, $\\cdots$, option N\n",
    "\n",
    "#### reward는 성공을 measure하는 값\n",
    "reward is  simply a measure of success\n",
    "\n",
    "예를 들어 고객들이 클릭을 하는지 안하는지,\n",
    "등록을 하는지 안하는지 여부로 정의 가능\n",
    "\n",
    "reward는 반드시 \n",
    "- quantitative 해서 수학적으로 track가능한 값이어야 하고,\n",
    "- 작은 값보다 reward가 큰 것이 better\n",
    "\n",
    "\n",
    "### Bandt Problem\n",
    "- a bandit which has a set of N arms =  N개의 옵션이 있는 문제\n",
    "- 하나의 옵션(pull any give arm)을 선택하면, 그에 따른 보상(reward)가 있다. (하지만 우리는 이 reward의 분포를 모름. riscky)\n",
    "- we do NOT start off kwing what the reward rates are for any of the arms. -> 이 값을 유추하기 위해서 arm들 당겨보는 것!\n",
    "\n",
    "우리가 하고 싶은건 가장 큰 reward 평균 값을 가지는 arm을 알아내고 싶음 \n",
    "But, 문제는 reward에 대한 정보가 너무 부족\n",
    "1) partial feedback문제 : 당겨본 arm에 대한 정보만 가지고 있지, 그 arm댕기느라 선택하지 못한 다른 arm에 대한 정보는 없음\n",
    "\n",
    "2) falling bhind문제 : 매번 best option을 뽑지 못하니까 lose reward -> 우리는 더 나은 arm 땡길수도 있었는데...\n",
    "\n",
    "\n",
    "### 즉 MAP (Multi Armed Bandit) 이란,\n",
    "- 정확한 승률을 알지 못하는 여러대의 slot machines을 가지고 도박을 할 때, 가장 많은 돈을 따고 싶지만 각 slot 마다 Reward확률 이다르고 어떤 slot의 reward가 가장 큰지 알지 못함.\n",
    "- 이런 상황에서 \" 어떤 순서로 어떤 slot을 얼마만큼 당겨야 가장 많은 reward (cumulative reward) 를 얻을 것인가\"의 문제!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b8e3f",
   "metadata": {
    "papermill": {
     "duration": 0.012069,
     "end_time": "2022-08-09T02:42:52.660896",
     "exception": false,
     "start_time": "2022-08-09T02:42:52.648827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Greedy Algorithm\n",
    "- 모든 slot machine 한번씩 play하고 reward가장 좋은 쪽을 exploit\n",
    "- 하지만 충분한 탐색이 이루어지지 않는다는 점\n",
    "\n",
    "## Epilon Greedy Algorithm\n",
    "- 1 - $\\epsilon$ 확률로는 exploitation\n",
    "-$\\epsilon$만큼은 exploration\n",
    "\n",
    "-> epsilon 모수가 eploration&exploitation trade-off를 조절 : 만약 이 값이 크면 explore 많이 하는것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04171390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:42:52.689811Z",
     "iopub.status.busy": "2022-08-09T02:42:52.689048Z",
     "iopub.status.idle": "2022-08-09T02:42:58.560553Z",
     "shell.execute_reply": "2022-08-09T02:42:58.559390Z"
    },
    "papermill": {
     "duration": 5.888652,
     "end_time": "2022-08-09T02:42:58.563628",
     "exception": false,
     "start_time": "2022-08-09T02:42:52.674976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6437568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:42:58.591366Z",
     "iopub.status.busy": "2022-08-09T02:42:58.590506Z",
     "iopub.status.idle": "2022-08-09T02:42:58.595251Z",
     "shell.execute_reply": "2022-08-09T02:42:58.594454Z"
    },
    "papermill": {
     "duration": 0.020881,
     "end_time": "2022-08-09T02:42:58.597362",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.576481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#arm의 갯수만큼 형성된 list 넣어주면, list에서 가장 큰 값을 고르고 그 index값 반환.\n",
    "def ind_max(x):\n",
    "    m = max(x)\n",
    "    return x.index(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf716c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:42:58.624994Z",
     "iopub.status.busy": "2022-08-09T02:42:58.624338Z",
     "iopub.status.idle": "2022-08-09T02:42:58.634649Z",
     "shell.execute_reply": "2022-08-09T02:42:58.633863Z"
    },
    "papermill": {
     "duration": 0.026613,
     "end_time": "2022-08-09T02:42:58.636890",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.610277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 설계\n",
    "# epsilon : a floating point number that tells us the frequency with which we should explore one of the available arms.\n",
    "# set epsilon = 0.1\n",
    "\n",
    "# counts : a vector of inteers of length N ( 각 arm을 몇번 당겼는지 담긴 벡터. 길이는 arm만큼이니까 N)\n",
    "# 이 예제에서는 Arm !, Arm 2 2개가 있고 각각 2번씩만 당겼으니까\n",
    "# counts = [2,2]\n",
    "\n",
    "# values : a verctor of floating point numbers \n",
    "#               that defines the average amount of reward we've gotten when playing N arms available to us.\n",
    "# 즉 보상의 평균, Arm 1 이 2번 당겼을 때 각각 1,0의 output을 내고, Arm 2rk 0,0의 output을 내면,\n",
    "# values = [0.5,0.0]\n",
    "import random\n",
    "\n",
    "#Epsilon Greedy 클래스 정의\n",
    "class EpsilonGreedy():\n",
    "    def __init__(self,epsilon,counts,values):\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = counts # 각 arm별당긴 횟수 저장 (크기는 arm갯수 만큼) : count <list>\n",
    "        self.values = values # 각 arm별 보상의 평균 (크기는 arm갯수 만큼): values <list>\n",
    "        return\n",
    "    \n",
    "    # provide explicit initialization method \n",
    "    #_counts, values변수를 알고리즘 돌아가기전 적절한 blank slate states로 리셋하는 함수\n",
    "    def initialize(self,n_arms):\n",
    "        self.counts = [0 for col in range(n_arms)]\n",
    "        self.values = [0.0 for col in range(n_arms)]\n",
    "        return\n",
    "\n",
    "\n",
    "    # select_arm 함수\n",
    "    # arm을 당기는 선택을 할 때 부르는 함수_ 당겨야 할 다음 arm의 index번호를 호출\n",
    "    # 아무 argument 없이 부르고, \n",
    "    # return : index of the next arm to pull\n",
    "\n",
    "\n",
    "    def select_arm(self):\n",
    "        if random.random() > self.epsilon: # 0 이상 1 미만의 숫자반환->클래스에서 정의된 episilon값보다 큰 값이 나오면,\n",
    "            return ind_max(self.values) # 지금 가지고 있는 values(reward의 평균)이 가장 큰 곳의 index뽑아 return\n",
    "        else:\n",
    "            return random.randrange(len(self.values)) #explore 탐색_ select the arm completely random_0~arm숫자 랜덤하게 index반환\n",
    "        \n",
    "\n",
    "    # upate 함수\n",
    "    # arm을 당기고 나면 reward를 얻는데, 이를 업데이터 해줘야해.\n",
    "    # input인자로는 alogirithm object, 방금 당긴 arm의 index, 방금 당긴 arm에 대한 reward\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] = self.counts[chosen_arm] + 1 #그 arm을 당긴 횟수를 저장하는 count값을 하나 올려줌\n",
    "        n = self.counts[chosen_arm]\n",
    "    \n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ((n-1)/float(n))*value + (1/float(n)) * reward #그 arm을 당긴 보상의 평균을 구하기\n",
    "        self.values[chosen_arm] = new_value #그 arm을 당긴 보상의 평균을 구한값 그 인덱스에 맞게 넣어주기\n",
    "        return\n",
    "\n",
    "    #추후에는 평균을 구하는게 아니라, average onine을 계산하기 위해서 alternative weighting scheme을 이용할 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9af10f",
   "metadata": {
    "papermill": {
     "duration": 0.012647,
     "end_time": "2022-08-09T02:42:58.662129",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.649482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### epsilon 결정하기_epsilon greedy의 한계점\n",
    "- $\\epsilon$ = 1.0이라면, 늘 완전히 random 하게 option을 선택. -> waste source acquiring data aoubt bad options\n",
    "- $\\epsilon$ = 0.0이라면, 가장 average reward높게 나온 option을 계속 당김 -> 새로운 것을 탐색할 수 없음.\n",
    "\n",
    "하지만 epsilon greedy algorithm은 \n",
    "- 최적 slot machine 찾은 후에도 계속해서 epsilon만큼은 랜덤하게 막 탐색한다는 한계점이 있다.\n",
    "- 입실론 만큼 확률로 sub optimal 한 나머지 slot machine을 무작위로 뽑아서, 전체 슬롯 머신 중 탐색하지 못하거나, 혹은 탐색을 덜해서 정보를 얻지 못한 슬롯 머신이 생길 가능성이큼.\n",
    "\n",
    "이를 보완하기 위해서 나온 알고리즘이 softmax!\n",
    "\n",
    "그 전에, 먼저 **Monte Carlo simulation**으로 random number generator사용해서 simulation해.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67c0b2",
   "metadata": {
    "papermill": {
     "duration": 0.012171,
     "end_time": "2022-08-09T02:42:58.687182",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.675011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CHAPTER4. Debugging Bandit Algorithms\n",
    "\n",
    "### Monte Carlo Simulations ARe Like Unit Tests for Bandit Algorithms\n",
    "\n",
    "bandit 알고리즘은 T시점에 select_arm()하면 그에 대한 reward가 있어서 update가 되야함.\n",
    "\n",
    "따라서 그냥 평균을 업데이트 하는 방법 대신, **Monte Carlo simulation 방법**을 택함.\n",
    "\n",
    "-> 2개이의 코드 짤 것임\n",
    "\n",
    "이 두 개의 코드가 함께 실제 상황에서 이 알고리즘이 어떻게 구현되는지 보여줌\n",
    "- bandit algorithm\n",
    "- a simulation of the bandit's arms that the algorithm as to select between\n",
    "\n",
    "simulation은 random number에 의해 돌아가므로 noise가 있음.\n",
    "-> 따라서 여러번 돌려서 develop an intuition for its behavior in different settins.\n",
    "\n",
    "\n",
    "### Simulating the Arms of a Bandit Problem\n",
    "hypothetical arm이 필요함\n",
    "보상이 click throughs ro user signups같은 0/1 문제일 때\n",
    "\n",
    "-> 우리는 이런 상황을 **\"a Bernoulli Arm\"** 이라고 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710df17",
   "metadata": {
    "papermill": {
     "duration": 0.012089,
     "end_time": "2022-08-09T02:42:58.711647",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.699558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 여러 Arm들 _ in here, I use Bernoulli arm\n",
    "- Bernoulli\n",
    "- Adversarial\n",
    "- normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b8fab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:42:58.738380Z",
     "iopub.status.busy": "2022-08-09T02:42:58.737678Z",
     "iopub.status.idle": "2022-08-09T02:42:58.742868Z",
     "shell.execute_reply": "2022-08-09T02:42:58.742129Z"
    },
    "papermill": {
     "duration": 0.021092,
     "end_time": "2022-08-09T02:42:58.744979",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.723887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bernoulli Arm\n",
    "# p확률만큼 성공을 reward로 줌.\n",
    "class BernoulliArm():\n",
    "    def __init__(self, p): # p는 arm으로 부터 1의 reward를 얻을 확률을 의미.\n",
    "        self.p = p\n",
    "  \n",
    "    def draw(self):\n",
    "        if random.random() > self.p:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ad4bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:42:58.772330Z",
     "iopub.status.busy": "2022-08-09T02:42:58.771638Z",
     "iopub.status.idle": "2022-08-09T02:42:58.777290Z",
     "shell.execute_reply": "2022-08-09T02:42:58.776473Z"
    },
    "papermill": {
     "duration": 0.022121,
     "end_time": "2022-08-09T02:42:58.779682",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.757561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AdversarialArm\n",
    "class AdversarialArm():\n",
    "    def __init__(self, t, active_start, active_end):\n",
    "        self.t = t\n",
    "        self.active_start = active_start\n",
    "        self.active_end = active_end\n",
    "  \n",
    "    def draw(self):\n",
    "        self.t = self.t + 1\n",
    "        if self.active_start <= self.t <= self.active_end:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a834cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:42:58.807418Z",
     "iopub.status.busy": "2022-08-09T02:42:58.806700Z",
     "iopub.status.idle": "2022-08-09T02:42:58.811876Z",
     "shell.execute_reply": "2022-08-09T02:42:58.811118Z"
    },
    "papermill": {
     "duration": 0.021177,
     "end_time": "2022-08-09T02:42:58.814033",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.792856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NormalArm\n",
    "import random\n",
    "\n",
    "class NormalArm():\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "  \n",
    "    def draw(self):\n",
    "        return random.gauss(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e0bf2",
   "metadata": {
    "papermill": {
     "duration": 0.012105,
     "end_time": "2022-08-09T02:42:58.839034",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.826929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf496768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:42:58.866292Z",
     "iopub.status.busy": "2022-08-09T02:42:58.865580Z",
     "iopub.status.idle": "2022-08-09T02:42:58.876030Z",
     "shell.execute_reply": "2022-08-09T02:42:58.875184Z"
    },
    "papermill": {
     "duration": 0.026931,
     "end_time": "2022-08-09T02:42:58.878391",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.851460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# core infrastructure and testing framework\n",
    "# initialize the bandit algorithm's setting from scratch \n",
    "#                             -> no prior knowledge about which arm is best.\n",
    "\n",
    "def test_algorithm(algo, arms, num_sims, horizon): \n",
    "    #어떤 알고리즘 쓸건지, arm들 list, 총 arm 당기는 횟수 T,\n",
    "    chosen_arms = [0.0 for i in range(num_sims * horizon)]\n",
    "    rewards = [0.0 for i in range(num_sims * horizon)]\n",
    "    cumulative_rewards = [0.0 for i in range(num_sims * horizon)]\n",
    "    sim_nums = [0.0 for i in range(num_sims * horizon)]\n",
    "    times = [0.0 for i in range(num_sims * horizon)]\n",
    "  \n",
    "\n",
    "    # 변수들 초기화\n",
    "    for sim in range(num_sims):\n",
    "        sim = sim + 1 \n",
    "        algo.initialize(len(arms)) \n",
    "        # count, values 리스트를 arm갯수만큼 0 원소 가지는 list만들기\n",
    "    \n",
    "        for t in range(horizon): # 이제 pull arm\n",
    "            t = t + 1\n",
    "            index = (sim - 1) * horizon + t - 1\n",
    "            sim_nums[index] = sim\n",
    "            times[index] = t \n",
    "            # horizon = 10이라면 1에서 10까지 각 time에서 num_sims(예를들어 100번) 만큼 arm 당겨보는거\n",
    "      \n",
    "            # select the arm_ 당겨야 할 다음 arm의 index번호를 호출\n",
    "            chosen_arm = algo.select_arm() \n",
    "            chosen_arms[index] = chosen_arm\n",
    "      \n",
    "            # 당겼을 때 reward 여기선 0 or 1\n",
    "            reward = arms[chosen_arms[index]].draw()\n",
    "            rewards[index] = reward\n",
    "      \n",
    "            # update cumulative reward _ 처음 당길 때 제외하고는 그전 값에 이번 reward 더해가면서\n",
    "            if t == 1:\n",
    "                cumulative_rewards[index] = reward\n",
    "            else:\n",
    "                cumulative_rewards[index] = cumulative_rewards[index - 1] + reward\n",
    "      \n",
    "            algo.update(chosen_arm, reward)\n",
    "  \n",
    "    return [sim_nums, times, chosen_arms, rewards, cumulative_rewards]\n",
    "    # 0에서 sim_num 몇번 index인지, 0에서 horizon 시점, 각 시점에서 선택된 arm, 각 시점에서 보상, 각 시점에서 갱신되는 cumulative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7075cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:42:58.905981Z",
     "iopub.status.busy": "2022-08-09T02:42:58.905297Z",
     "iopub.status.idle": "2022-08-09T02:43:39.392399Z",
     "shell.execute_reply": "2022-08-09T02:43:39.391340Z"
    },
    "papermill": {
     "duration": 40.504519,
     "end_time": "2022-08-09T02:43:39.395287",
     "exception": false,
     "start_time": "2022-08-09T02:42:58.890768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.BernoulliArm object at 0x7f7fdc192350>, <__main__.BernoulliArm object at 0x7f7fdc192410>, <__main__.BernoulliArm object at 0x7f7fdc192450>, <__main__.BernoulliArm object at 0x7f7fdc192490>, <__main__.BernoulliArm object at 0x7f7fdc1924d0>]\n",
      "Best arm is 2\n"
     ]
    }
   ],
   "source": [
    "#execfile(\"core.py\")\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# set up an array of Arm objects\n",
    "means = [0.1, 0.1, 0.1, 0.1, 0.9] # 5개의 arm 생성. 4개는 0.1 reward, 1개는 0.9 reward.\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "print(arms)\n",
    "print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "f = open(\"D:\\coding\\bandit\\algorithms\\epsilon_greedy\\standard_results.tsv\", \"w\")\n",
    "\n",
    "for epsilon in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    algo = EpsilonGreedy(epsilon, [], [])\n",
    "    algo.initialize(n_arms)\n",
    "    results = test_algorithm(algo, arms, 5000, 250)\n",
    "    for i in range(len(results[0])):\n",
    "        f.write(str(epsilon) + \"\\t\")\n",
    "        f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7379f6b",
   "metadata": {
    "papermill": {
     "duration": 0.01211,
     "end_time": "2022-08-09T02:43:39.420575",
     "exception": false,
     "start_time": "2022-08-09T02:43:39.408465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb6236",
   "metadata": {
    "papermill": {
     "duration": 0.01208,
     "end_time": "2022-08-09T02:43:39.445119",
     "exception": false,
     "start_time": "2022-08-09T02:43:39.433039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 참고 책에 없음 _ annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaec212f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:43:39.471803Z",
     "iopub.status.busy": "2022-08-09T02:43:39.471045Z",
     "iopub.status.idle": "2022-08-09T02:43:39.481852Z",
     "shell.execute_reply": "2022-08-09T02:43:39.480833Z"
    },
    "papermill": {
     "duration": 0.026859,
     "end_time": "2022-08-09T02:43:39.484175",
     "exception": false,
     "start_time": "2022-08-09T02:43:39.457316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#annealing\n",
    "import random\n",
    "import math\n",
    "\n",
    "def ind_max(x):\n",
    "    m = max(x)\n",
    "    return x.index(m)\n",
    "\n",
    "class AnnealingEpsilonGreedy():\n",
    "    def __init__(self, counts, values):\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        return\n",
    "    \n",
    "    def initialize(self, n_arms):\n",
    "        self.counts = [0 for col in range(n_arms)]\n",
    "        self.values = [0.0 for col in range(n_arms)]\n",
    "        return\n",
    "    \n",
    "    def select_arm(self):\n",
    "        t = sum(self.counts) + 1 #지금까지 arm을 당긴 횟수\n",
    "        epsilon = 1 / math.log(t + 0.0000001) #조금씩 epsilon을 줄여가면서\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            return ind_max(self.values)\n",
    "        else:\n",
    "            return random.randrange(len(self.values))\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] = self.counts[chosen_arm] + 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        \n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward\n",
    "        self.values[chosen_arm] = new_value\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c1ae2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:43:39.512476Z",
     "iopub.status.busy": "2022-08-09T02:43:39.512048Z",
     "iopub.status.idle": "2022-08-09T02:43:47.722173Z",
     "shell.execute_reply": "2022-08-09T02:43:47.720955Z"
    },
    "papermill": {
     "duration": 8.227795,
     "end_time": "2022-08-09T02:43:47.725006",
     "exception": false,
     "start_time": "2022-08-09T02:43:39.497211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best arm is 2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "means = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "my_algo = AnnealingEpsilonGreedy([], [])\n",
    "my_algo.initialize(n_arms)\n",
    "results = test_algorithm(my_algo, arms, 5000, 250)\n",
    "\n",
    "f = open(\"D:\\coding\\bandit\\algorithms\\epsilon_greedy\\annealing_results.tsv\", \"w\")\n",
    "for i in range(len(results[0])):\n",
    "    f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73f8ee",
   "metadata": {
    "papermill": {
     "duration": 0.012431,
     "end_time": "2022-08-09T02:43:47.749895",
     "exception": false,
     "start_time": "2022-08-09T02:43:47.737464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CHAPTER 5. The Softmax Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9745d",
   "metadata": {
    "papermill": {
     "duration": 0.012166,
     "end_time": "2022-08-09T02:43:47.774511",
     "exception": false,
     "start_time": "2022-08-09T02:43:47.762345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "MAB 알고리즘 중 하나인 Softmax 알고리즘은\n",
    "각 슬롯머신의 관측된 보상 획득률에 비례하는 확\n",
    "률로 다음에 시행할 슬롯머신을 선택한다. 시행 전\n",
    "에 온도($\\tau$)라고 불리는 변수를 직접 설정해주어야\n",
    "하며 $\\tau$값이 높을수록 탐색단계 비중이 높은 알고\n",
    "리즘이 되고, $\\tau$값이 낮을수록 활용단계 비중이 높은\n",
    "알고리즘이 된다. 따라서 초기 시행에는 $\\tau$값을 높여\n",
    "탐색을 진행하고 이후 $\\tau$값을 낮춰 활용단계로 전환\n",
    "하는 방식으로 Softmax 알고리즘을 사용하게 된다. 하지만 알맞은 초기 $\\tau$값과 목표 $\\tau$값의 수치, 초기 $\\tau$값에서 목표 $\\tau$값까지 변화하는 정도를 결정하기\n",
    "어렵다는 단점이 존재한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ed82f",
   "metadata": {
    "papermill": {
     "duration": 0.012053,
     "end_time": "2022-08-09T02:43:47.799466",
     "exception": false,
     "start_time": "2022-08-09T02:43:47.787413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction the Softmax Alorithm\n",
    "\n",
    "위에서 언급했듯이,\n",
    "입실론 방법에서는 2가지 단점이 존재\n",
    "\n",
    "- 만약 두개의 보상률의 차이가 작다면 (예를들어 $\\epsilon= 0.1$일 때) , 10%의 확률로 explore하는 것은 무엇이 더 좋은 arm인지 찾기에 너무 적음. 더 많이 탐색해야함.\n",
    "- 반대로, 만약 보상률의 차이가 확연하다면, 결국 더 좋지 않은 arm을 탐색하는 비율도 같으니까 시간 낭비!\n",
    "\n",
    "\n",
    "따라서 우리는 **haphazard exploration**이 아닌, **structured explration**이 필요함.\n",
    "\n",
    "그 중 하나가 **softmax algorithm**\n",
    "\n",
    "Softmax Algorithm 을 먼저 쉽게 이해하기 위해서,\n",
    "**추정값에 비례**해서 arm을 선택한다고 생각해보자.\n",
    "\n",
    "예를 들어서 arm A, arm B가 있을 때,\n",
    "두 개의 성공 확률(즉 보상을 받을 확률)이 각각 rA, rB라면,\n",
    "Arm A는 rA / (rA + rB)의 확률로, Arm A는 rB / (rA + rB)의 확률로 선택하는 것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527190b",
   "metadata": {
    "papermill": {
     "duration": 0.012578,
     "end_time": "2022-08-09T02:43:47.824339",
     "exception": false,
     "start_time": "2022-08-09T02:43:47.811761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implementing the Softmax Algorithm\n",
    "\n",
    "하지만 우리가 쓰는 알고리즘은 이렇게 간단하지는 않음.\n",
    "\n",
    "2가지를 바꿔야함.\n",
    "\n",
    "1) calculate a different scale for reward rates by exponentiating our estimates of rA and rB,\n",
    "  \n",
    "  - Arm A는 $\\exp(rA) / (\\exp(rA)+ \\exp(rB))$의 확률로, \n",
    "  \n",
    "  - Arm A는 $\\exp(rB) / (\\exp(rA)+ \\exp(rB))$의 확률로 선택\n",
    "  \n",
    "  \n",
    "2) temperatur parameter $\\tau$\n",
    "\n",
    " - Arm A는 $\\exp(rA/\\tau) / (\\exp(rA/\\tau)+ \\exp(rB/\\tau))$의 확률로, \n",
    "\n",
    " - Arm A는 $\\exp(rB/\\tau) / (\\exp(rA/\\tau)+ \\exp(rB/\\tau))$의 확률로 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb7d12",
   "metadata": {
    "papermill": {
     "duration": 0.012106,
     "end_time": "2022-08-09T02:43:47.848984",
     "exception": false,
     "start_time": "2022-08-09T02:43:47.836878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### tau의 역할\n",
    "tau는 slotmachine을 선택하는 극단적인 방법으로 정의되는 continuum을 따라서 softmax 알고리즘의 동작을 이동하게함.\n",
    "\n",
    "예를 들어 \n",
    "- $\\tau = 0.0$이라면, 추정치가 가장 높은 slotmachine 선택 ( fully deterministic )\n",
    "\n",
    "- $\\tau = \\infty$이라면, 입실론 그리디 알고리즘과 같음 ( fully randomly selected )\n",
    "\n",
    "따라서 reward가 좋은 쪽으로 얼마나 더 뽑을 확률을 줄것인지를 tau가 결정하게 함.\n",
    "\n",
    "\n",
    "> 예를 들어 arm들의 reward 간에 clear difference 있을 때\n",
    "\n",
    "                  (책에서는 0.1, 0.1, 0.1, 0.1, 0.9),\n",
    "\n",
    "     tau가 0.1이라도 처음부터 좋은 성과 냄. ( explore 많이 할 필요 없음. 확연하니까 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd17edbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:43:47.875961Z",
     "iopub.status.busy": "2022-08-09T02:43:47.875355Z",
     "iopub.status.idle": "2022-08-09T02:43:47.933228Z",
     "shell.execute_reply": "2022-08-09T02:43:47.932349Z"
    },
    "papermill": {
     "duration": 0.074323,
     "end_time": "2022-08-09T02:43:47.935761",
     "exception": false,
     "start_time": "2022-08-09T02:43:47.861438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#softmax/standard.py\n",
    "import math\n",
    "import random\n",
    "\n",
    "def categorical_draw(probs): #이게 어떻게 보상의 확률로 비례해서 당길 arm의 index를 주는건지 모르겠음.\n",
    "    z = random.random()\n",
    "    cum_prob = 0.0\n",
    "    for i in range(len(probs)):\n",
    "        prob = probs[i]\n",
    "        cum_prob += prob\n",
    "        if cum_prob > z:\n",
    "            return i\n",
    "  \n",
    "    return len(probs) - 1\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self, temperature, counts, values):\n",
    "        self.temperature = temperature\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        return\n",
    "  \n",
    "    def initialize(self, n_arms):\n",
    "        self.counts = [0 for col in range(n_arms)]\n",
    "        self.values = [0.0 for col in range(n_arms)]\n",
    "        return\n",
    "  \n",
    "    def select_arm(self):\n",
    "        # z : 보상을 tau로 나누고 rescale한 것의 합. 위 예에서는 (\\exp(rA/\\tau)+ \\exp(rB/\\tau))\n",
    "        z = sum([math.exp(v / self.temperature) for v in self.values])\n",
    "        \n",
    "        # 위에서 구한 보상의 rescaling들의 합을 분모로, 각각 보상 rescal한것을 분자로 하여 list생성.\n",
    "        probs = [math.exp(v / self.temperature) / z for v in self.values]\n",
    "        return categorical_draw(probs)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] = self.counts[chosen_arm] + 1\n",
    "        n = self.counts[chosen_arm]\n",
    "    \n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward\n",
    "        self.values[chosen_arm] = new_value\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e759cf66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:43:47.963211Z",
     "iopub.status.busy": "2022-08-09T02:43:47.962505Z",
     "iopub.status.idle": "2022-08-09T02:44:50.890742Z",
     "shell.execute_reply": "2022-08-09T02:44:50.889554Z"
    },
    "papermill": {
     "duration": 62.945356,
     "end_time": "2022-08-09T02:44:50.893640",
     "exception": false,
     "start_time": "2022-08-09T02:43:47.948284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best arm is 2\n"
     ]
    }
   ],
   "source": [
    "# softmax/test_standard.py\n",
    "# tau값이 작을 수록 안정성이 높게 exploit하고 tau가 클수록 explore하니까\n",
    "# tau를 처음에는 크게 했다가 점점 작게 만들어 줌으로써 \n",
    "# 초반에는 탐색 - > 후반에는 exploit\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "means = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "\n",
    "f = open(\"D:\\coding\\bandit\\algorithms\\softmax\\standard_softmax_results.tsv\", \"w\")\n",
    "\n",
    "for temperature in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    algo = Softmax(temperature, [], [])\n",
    "    algo.initialize(n_arms)\n",
    "    results = test_algorithm(algo, arms, 5000, 250)\n",
    "    for i in range(len(results[0])):\n",
    "        f.write(str(temperature) + \"\\t\")\n",
    "        f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb114e",
   "metadata": {
    "papermill": {
     "duration": 0.012072,
     "end_time": "2022-08-09T02:44:50.918374",
     "exception": false,
     "start_time": "2022-08-09T02:44:50.906302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Annealing이란\n",
    "- *annealing* is the process of decreasing the temperature in a Softmax algorithm over time.\n",
    "- *annealing* is a process of modifying a baindit algorithm's behavior so that it will explore less over time.\n",
    "\n",
    "즉 어닐링이란 시간이 흐르면서 온도 parameter인 tau의 값을 점점 줄여감으로써,\n",
    "\n",
    "탐색의 비율을 줄이고 exploit을 많이 하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e001dff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:44:50.944640Z",
     "iopub.status.busy": "2022-08-09T02:44:50.944237Z",
     "iopub.status.idle": "2022-08-09T02:44:50.955783Z",
     "shell.execute_reply": "2022-08-09T02:44:50.954719Z"
    },
    "papermill": {
     "duration": 0.027479,
     "end_time": "2022-08-09T02:44:50.958237",
     "exception": false,
     "start_time": "2022-08-09T02:44:50.930758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# softmax/annealing.py\n",
    "import math\n",
    "import random\n",
    "\n",
    "def categorical_draw(probs):\n",
    "    z = random.random()\n",
    "    cum_prob = 0.0\n",
    "    for i in range(len(probs)):\n",
    "        prob = probs[i]\n",
    "        cum_prob += prob\n",
    "        if cum_prob > z:\n",
    "            return i\n",
    "  \n",
    "    return len(probs) - 1\n",
    "\n",
    "class AnnealingSoftmax:\n",
    "    def __init__(self, counts, values):\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        return\n",
    "  \n",
    "    def initialize(self, n_arms):\n",
    "        self.counts = [0 for col in range(n_arms)]\n",
    "        self.values = [0.0 for col in range(n_arms)]\n",
    "        return\n",
    "  \n",
    "    def select_arm(self):\n",
    "        t = sum(self.counts) + 1\n",
    "        temperature = 1 / math.log(t + 0.0000001) #조금씩 tau 줄여가면서 ( exploit more )\n",
    "\n",
    "        z = sum([math.exp(v / temperature) for v in self.values])\n",
    "        probs = [math.exp(v / temperature) / z for v in self.values]\n",
    "        return categorical_draw(probs)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] = self.counts[chosen_arm] + 1\n",
    "        n = self.counts[chosen_arm]\n",
    "    \n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward\n",
    "        self.values[chosen_arm] = new_value\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1845e508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:44:50.985298Z",
     "iopub.status.busy": "2022-08-09T02:44:50.984875Z",
     "iopub.status.idle": "2022-08-09T02:45:03.014151Z",
     "shell.execute_reply": "2022-08-09T02:45:03.013163Z"
    },
    "papermill": {
     "duration": 12.045497,
     "end_time": "2022-08-09T02:45:03.016672",
     "exception": false,
     "start_time": "2022-08-09T02:44:50.971175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best arm is 2\n"
     ]
    }
   ],
   "source": [
    "# softmax/test_annealing.py\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "means = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "algo = AnnealingSoftmax([], [])\n",
    "algo.initialize(n_arms)\n",
    "results = test_algorithm(algo, arms, 5000, 250)\n",
    "\n",
    "\n",
    "f = open(\"D:\\coding\\bandit\\algorithms\\softmax\\annealing_softmax_results.tsv\", \"w\")\n",
    "for i in range(len(results[0])):\n",
    "    f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008a1ef",
   "metadata": {
    "papermill": {
     "duration": 0.012191,
     "end_time": "2022-08-09T02:45:03.041372",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.029181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CHAPTER 6.UCB - The Upper Confidence Bound Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfbd01c",
   "metadata": {
    "papermill": {
     "duration": 0.012212,
     "end_time": "2022-08-09T02:45:03.066050",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.053838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "보상 획득률에 더하여 전체 시행횟수와 각 슬롯머신의 시행횟수를 이용하는 알고리즘이며, 사전에 설정해야 하는 변수가 없는 특징이 있다. UCB 알고리즘은 충\n",
    "분한 시행 후에는 MAB 알고리즘 중 가장 높은 보상을 얻는 장점이 있으나, 충분한 시행 횟수를 알아내기 어렵고 시행 초기에 평균 보상 획득률의 변동\n",
    "이 상대적으로 매우 크기 때문에, 시행 기간이 짧다면 그 결과를 신뢰하기 어렵다는 단점이 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380786a",
   "metadata": {
    "papermill": {
     "duration": 0.012931,
     "end_time": "2022-08-09T02:45:03.091353",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.078422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction the UCB Algorithm\n",
    "\n",
    "\n",
    "#### epsilong greedy & Softmax 공통점\n",
    " - default choice로 최근 highest estimated value 갖는 arm 선택\n",
    " - 가끔씩 (입실론 그리디는 $\\epsilon$만큼, softmax는 각 arm들의 estimated value만큼 비례해서) best아닌 것도 explore함.\n",
    "  \n",
    "\n",
    "$\\rightarrow$ 지금까지 살펴본\n",
    "greedy와 softmax 알고리즘은 \" 지금까지 얻은 reward \" 값에 기반하여 \n",
    "뽑을 arm을 결정함.\n",
    "\n",
    "이미 알고 있는 사실에 초점을 맞춤\n",
    "\n",
    "따라서 \" how much they know \" 얼마나 알고 있는지에 대해서는 신경쓰지 않음\n",
    "\n",
    "그 결과,\n",
    "reward가 적게 나온 arm에대해서는 underexplore하게 됨.\n",
    "(arm이 bestarm보다 reward좋지 않으니까 뽑힐 확률이 더 적음.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359f9d2",
   "metadata": {
    "papermill": {
     "duration": 0.013412,
     "end_time": "2022-08-09T02:45:03.117262",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.103850",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c553cf76",
   "metadata": {
    "papermill": {
     "duration": 0.013008,
     "end_time": "2022-08-09T02:45:03.143687",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.130679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### UCB알고리즘의 장단점\n",
    "\n",
    "#### 장점\n",
    "- 설정해야할 hyperparameter가 없음\n",
    "( 따라서 현실적으로 사용하기 좋음 ) \n",
    "\n",
    "- 랜덤 탐색을 하지 않음.\n",
    "\n",
    "  just 최적이 될 수 있을 만한 slot머신을 선택할 가능성을 수치로 계산하여 이를 바탕으로 선택 결정\n",
    "\n",
    "  ( 따라서 어떻게 행동하는지 알 수 있음 )\n",
    "  \n",
    "\n",
    "#### 단점\n",
    "- 매 round마다 수치 값을 갱신 하여야 함.\n",
    "- UCB 계산을 위해 empirical mean이 필수적이기 때문에, 처음에 모든 slot machine을 한번씩 다 탐색해야함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c9c8a",
   "metadata": {
    "papermill": {
     "duration": 0.013358,
     "end_time": "2022-08-09T02:45:03.169944",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.156586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 왜 UCB를 사용해야 할까?\n",
    "#### UCB can make decisions to explore that are driven by our confidence in the estimated value of the arms we've selected.\n",
    "\n",
    "UCB는 우리가 고른 arm들에 대한 추정 보상의 신뢰구간을 사용해서 explore할지 결정.\n",
    "\n",
    "-> arm들의 보상의 confidence가 중요한 이유 : \n",
    "\n",
    " : rewards는 noise가 있기 때문!\n",
    " \n",
    "epsilon greedy, softmax 알고리즘은 are **NOT** robust to this noise.\n",
    "\n",
    "randomness가 있기 때문에 우연히 negative experience하게 되면, 치명적으로 영향 받을 수 있음\n",
    "\n",
    "//\n",
    "\n",
    "하지만 UCB는 전혀 randomness가 없음!\n",
    "그 대신 UCB는 모든 arm들에 대해서 **추정된 보상의 값들의 평가에 대한 confidence를 keep track** 함 : )\n",
    "\n",
    "-> 이를 위해서, we need to have some metric of how much we know about each arm\n",
    "\n",
    "( 우리가 각각 arm들에 대해서 얼마나 아는지 재는 척도가 필요 : 우리가 **각각 arm들을 얼마나 당겼는지 횟수** 이용 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3059303",
   "metadata": {
    "papermill": {
     "duration": 0.012334,
     "end_time": "2022-08-09T02:45:03.195565",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.183231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UCB1\n",
    "\n",
    "이 책에서는 UCB1에 대해서만 다룸\n",
    "\n",
    "UCB1은 the maximum possible reward가 1이라는 가정 필요.\n",
    "\n",
    "(만약 그렇지 않다면, rescale all of rewards to lie between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5abecab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:45:03.222850Z",
     "iopub.status.busy": "2022-08-09T02:45:03.222409Z",
     "iopub.status.idle": "2022-08-09T02:45:03.236243Z",
     "shell.execute_reply": "2022-08-09T02:45:03.234859Z"
    },
    "papermill": {
     "duration": 0.030288,
     "end_time": "2022-08-09T02:45:03.238803",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.208515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ucb/ucb1.py \n",
    "import math\n",
    "\n",
    "def ind_max(x):\n",
    "    m = max(x)\n",
    "    return x.index(m)\n",
    "\n",
    "class UCB1():\n",
    "    def __init__(self, counts, values): #결정해야 할 hyperparameters가 존재하지 않음.\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        return\n",
    "  \n",
    "    def initialize(self, n_arms):\n",
    "        self.counts = [0 for col in range(n_arms)]\n",
    "        self.values = [0.0 for col in range(n_arms)]\n",
    "        return\n",
    "  \n",
    "    # to ensure that 적어도 한번씩은 모든 arm들을 땡겨야한다\n",
    "    # 이는 cold start가 없도록 해줌.\n",
    "    # it ensures that UCB knows little bit all about the arms 모든 arm에 대해서 조금씩은 알고 있음.\n",
    "    # 따라서 완전히 분명히 효율이 떨어지는 arm에 대해서 처음부터 explore 안하도록 할 수 있음.\n",
    "    # 하지만 적어도 모든 arm을 한번씩은 당겨봐야한다는 단점\n",
    "    def select_arm(self):\n",
    "        n_arms = len(self.counts)\n",
    "        for arm in range(n_arms):\n",
    "            if self.counts[arm] == 0: # 적어도 한번씩은 모든 arm들을 땡겨야한다\n",
    "                return arm\n",
    "\n",
    "        ucb_values = [0.0 for arm in range(n_arms)]\n",
    "        total_counts = sum(self.counts)\n",
    "        for arm in range(n_arms):\n",
    "            bonus = math.sqrt((2 * math.log(total_counts)) / float(self.counts[arm]))\n",
    "             # 시간이 흐를 수록 이 term의 영향력은 작아짐. (로그로 서서히 증가하므로),\n",
    "             # 당기는 횟수가 많을 수록 작아짐 (즉 적게 당긴 arm일 수록 뽑힐 확률이 증가 )\n",
    "                # 얼마나 우리가 덜 아는지를 반영_ 즉 다른거 보다 little worse해 보여도 모르는 것에 대해서 알고자함.\n",
    "            ucb_values[arm] = self.values[arm] + bonus # reward의 평균에 penalty항이 같이 붙음.\n",
    "        return ind_max(ucb_values)\n",
    "  \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] = self.counts[chosen_arm] + 1\n",
    "        n = self.counts[chosen_arm]\n",
    "\n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward\n",
    "        self.values[chosen_arm] = new_value\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f750db4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:45:03.267076Z",
     "iopub.status.busy": "2022-08-09T02:45:03.266442Z",
     "iopub.status.idle": "2022-08-09T02:45:03.270960Z",
     "shell.execute_reply": "2022-08-09T02:45:03.270111Z"
    },
    "papermill": {
     "duration": 0.021169,
     "end_time": "2022-08-09T02:45:03.273106",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.251937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ucb/test_ucb1.py\n",
    "# import random\n",
    "\n",
    "# random.seed(1)\n",
    "# means = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "# n_arms = len(means)\n",
    "# random.shuffle(means)\n",
    "# arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "# print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "# algo = UCB1([], [])\n",
    "# algo.initialize(n_arms)\n",
    "# results = test_algorithm(algo, arms, 5000, 250)\n",
    "\n",
    "# f = open(\"D:\\coding\\bandit\\algorithms\\ucb\\ucb1_results.tsv\", \"w\")\n",
    "\n",
    "# for i in range(len(results[0])):\n",
    "#     f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]) + \"\\n\")\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529071b",
   "metadata": {
    "papermill": {
     "duration": 0.012846,
     "end_time": "2022-08-09T02:45:03.298893",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.286047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "책 figure 6-1설명 참조.\n",
    "그래프 그리는법 알아보기\n",
    "\n",
    "In factthis curiosity bonus means that UCB can behave in very surprising ways. For example, consider the plot shown in Fugure 6-1 of UCB's chances of selecting the right arm at any given point in time.\n",
    "\n",
    "This graph looks very noisy compared with he graphs we've shown for the epsilon Greedy and Softmax algorithm. As we noted ealier, UCB does not use any randomness when selecting arms. so where is teh noise comming from?\n",
    "And Why is it so striking compared with the randomized algorithms we described earlier?\n",
    "\n",
    "The answer is surprising and reveals why the curiosity bonus that UCB has can behave in an non-intuitive ways : The little dips you see in this graph come from UCB backpedaling and experimenting with inferior arms because it comes to the conclusion that it knows too little about those arms. This backpedaling matters less and less over time, but is is alwyas present inUCB's behavior, which means that UCB doe snot become a strictly greedy algorithm even if you have a huge amount of data.\n",
    "\n",
    "At fist this backpdaling may seem troubling.\n",
    "To convince you that UCB is often very effective despite this counter intuitive tendency to oscillate back into exploring inferior arms, we need to explicitly compare UCB with the other algorithms we have studied so far.\n",
    "\n",
    "This is quite easy to do, because we can simply pool all of the simulation results we ahve gathered so far and treat them like a single unit for analysis. In the next section we wak though the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde90336",
   "metadata": {
    "papermill": {
     "duration": 0.012887,
     "end_time": "2022-08-09T02:45:03.324769",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.311882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 참고 책에 없음 _ucb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc536b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:45:03.353315Z",
     "iopub.status.busy": "2022-08-09T02:45:03.352582Z",
     "iopub.status.idle": "2022-08-09T02:45:03.417491Z",
     "shell.execute_reply": "2022-08-09T02:45:03.416369Z"
    },
    "papermill": {
     "duration": 0.082493,
     "end_time": "2022-08-09T02:45:03.419788",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.337295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ucb2\n",
    "import math\n",
    "\n",
    "def ind_max(x):\n",
    "    m = max(x)\n",
    "    return x.index(m)\n",
    "\n",
    "class UCB2(object):\n",
    "    def __init__(self, alpha, counts, values):\n",
    "\n",
    "    # UCB2 algorithm. Implementation of the slides at:\n",
    "    # http://lane.compbio.cmu.edu/courses/slides_ucb.pdf\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        self.__current_arm = 0\n",
    "        self.__next_update = 0\n",
    "        return\n",
    "  \n",
    "    def initialize(self, n_arms):\n",
    "        self.counts = [0 for col in range(n_arms)]\n",
    "        self.values = [0.0 for col in range(n_arms)]\n",
    "        self.r = [0 for col in range(n_arms)]\n",
    "        self.__current_arm = 0\n",
    "        self.__next_update = 0\n",
    "  \n",
    "    def __bonus(self, n, r):\n",
    "        tau = self.__tau(r)\n",
    "        bonus = math.sqrt((1. + self.alpha) * math.log(math.e * float(n) / tau) / (2 * tau))\n",
    "        return bonus\n",
    "  \n",
    "    def __tau(self, r):\n",
    "        return int(math.ceil((1 + self.alpha) ** r))\n",
    "  \n",
    "    def __set_arm(self, arm):\n",
    "\n",
    "    # When choosing a new arm, make sure we play that arm for\n",
    "    # tau(r+1) - tau(r) episodes.\n",
    "\n",
    "        self.__current_arm = arm\n",
    "        self.__next_update += max(1, self.__tau(self.r[arm] + 1) - self.__tau(self.r[arm]))\n",
    "        self.r[arm] += 1\n",
    "  \n",
    "    def select_arm(self):\n",
    "        n_arms = len(self.counts)\n",
    "    \n",
    "    # play each arm once\n",
    "        for arm in range(n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                self.__set_arm(arm)\n",
    "                return arm\n",
    "    \n",
    "    # make sure we aren't still playing the previous arm.\n",
    "        if self.__next_update > sum(self.counts):\n",
    "            return self.__current_arm\n",
    "    \n",
    "        ucb_values = [0.0 for arm in range(n_arms)]\n",
    "        total_counts = sum(self.counts)\n",
    "        for arm in xrange(n_arms):\n",
    "            bonus = self.__bonus(total_counts, self.r[arm])\n",
    "            ucb_values[arm] = self.values[arm] + bonus\n",
    "    \n",
    "        chosen_arm = ind_max(ucb_values)\n",
    "        self.__set_arm(chosen_arm)\n",
    "        return chosen_arm\n",
    "  \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] = self.counts[chosen_arm] + 1\n",
    "        n = self.counts[chosen_arm]\n",
    "    \n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward\n",
    "        self.values[chosen_arm] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7fd28ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:45:03.446994Z",
     "iopub.status.busy": "2022-08-09T02:45:03.446611Z",
     "iopub.status.idle": "2022-08-09T02:45:03.452158Z",
     "shell.execute_reply": "2022-08-09T02:45:03.450695Z"
    },
    "papermill": {
     "duration": 0.022303,
     "end_time": "2022-08-09T02:45:03.454753",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.432450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ucb/test_ucb2.py\n",
    "# import random\n",
    "\n",
    "# random.seed(1)\n",
    "# means = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "# n_arms = len(means)\n",
    "# random.shuffle(means)\n",
    "# arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "# print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "# for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "#     algo = UCB2(alpha, [], [])\n",
    "#     algo.initialize(n_arms)\n",
    "#     results = test_algorithm(algo, arms, 5000, 250)\n",
    "    \n",
    "#     f = open(\"D:\\coding\\bandit\\algorithms\\ucb\\ucbucb2_results.tsv\", \"w\")\n",
    "\n",
    "#     #f = open(\"D:\\coding\\bandit\\algorithms\\ucbucb2_results_%s.tsv\" % alpha, \"w\")\n",
    "\n",
    "#     for i in range(len(results[0])):\n",
    "#         f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]))\n",
    "#         f.write(\"\\t%s\\n\" % alpha)\n",
    "\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d683d",
   "metadata": {
    "papermill": {
     "duration": 0.012164,
     "end_time": "2022-08-09T02:45:03.479535",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.467371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 참고 책에 없음 _exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70f57115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:45:03.507435Z",
     "iopub.status.busy": "2022-08-09T02:45:03.506696Z",
     "iopub.status.idle": "2022-08-09T02:45:03.518373Z",
     "shell.execute_reply": "2022-08-09T02:45:03.517399Z"
    },
    "papermill": {
     "duration": 0.028223,
     "end_time": "2022-08-09T02:45:03.520614",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.492391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def categorical_draw(probs):\n",
    "    z = random.random()\n",
    "    cum_prob = 0.0\n",
    "    for i in range(len(probs)):\n",
    "        prob = probs[i]\n",
    "        cum_prob += prob\n",
    "        if cum_prob > z:\n",
    "            return i\n",
    "\n",
    "    return len(probs) - 1\n",
    "\n",
    "class Exp3():\n",
    "    def __init__(self, gamma, weights):\n",
    "        self.gamma = gamma\n",
    "        self.weights = weights\n",
    "        return\n",
    "  \n",
    "    def initialize(self, n_arms):\n",
    "        self.weights = [1.0 for i in range(n_arms)]\n",
    "        return\n",
    "  \n",
    "    def select_arm(self):\n",
    "        n_arms = len(self.weights)\n",
    "        total_weight = sum(self.weights)\n",
    "        probs = [0.0 for i in range(n_arms)]\n",
    "        for arm in range(n_arms):\n",
    "            probs[arm] = (1 - self.gamma) * (self.weights[arm] / total_weight)\n",
    "            probs[arm] = probs[arm] + (self.gamma) * (1.0 / float(n_arms))\n",
    "        return categorical_draw(probs)\n",
    "  \n",
    "    def update(self, chosen_arm, reward):\n",
    "        n_arms = len(self.weights)\n",
    "        total_weight = sum(self.weights)\n",
    "        probs = [0.0 for i in range(n_arms)]\n",
    "        for arm in range(n_arms):\n",
    "            probs[arm] = (1 - self.gamma) * (self.weights[arm] / total_weight)\n",
    "            probs[arm] = probs[arm] + (self.gamma) * (1.0 / float(n_arms))\n",
    "    \n",
    "        x = reward / probs[chosen_arm]\n",
    "    \n",
    "        growth_factor = math.exp((self.gamma / n_arms) * x)\n",
    "        self.weights[chosen_arm] = self.weights[chosen_arm] * growth_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "934c56b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:45:03.547647Z",
     "iopub.status.busy": "2022-08-09T02:45:03.546937Z",
     "iopub.status.idle": "2022-08-09T02:46:26.829793Z",
     "shell.execute_reply": "2022-08-09T02:46:26.828612Z"
    },
    "papermill": {
     "duration": 83.299541,
     "end_time": "2022-08-09T02:46:26.832642",
     "exception": false,
     "start_time": "2022-08-09T02:45:03.533101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best arm is 2\n"
     ]
    }
   ],
   "source": [
    "# exp3/test_exp3.py\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "means = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "\n",
    "f = open(\"D:\\coding\\bandit\\algorithms\\exp3\\exp3_results.tsv\", \"w\")\n",
    "\n",
    "for exp3_gamma in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    algo = Exp3(exp3_gamma, [])\n",
    "    algo.initialize(n_arms)\n",
    "    results = test_algorithm(algo, arms, 5000, 250)\n",
    "    for i in range(len(results[0])):\n",
    "        f.write(str(exp3_gamma) + \"\\t\")\n",
    "        f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eedfb5",
   "metadata": {
    "papermill": {
     "duration": 0.012416,
     "end_time": "2022-08-09T02:46:26.858034",
     "exception": false,
     "start_time": "2022-08-09T02:46:26.845618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 참고 책에 없음 _Hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "944baeb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:46:26.885667Z",
     "iopub.status.busy": "2022-08-09T02:46:26.884850Z",
     "iopub.status.idle": "2022-08-09T02:46:26.895946Z",
     "shell.execute_reply": "2022-08-09T02:46:26.895175Z"
    },
    "papermill": {
     "duration": 0.027571,
     "end_time": "2022-08-09T02:46:26.898330",
     "exception": false,
     "start_time": "2022-08-09T02:46:26.870759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def categorical_draw(probs):\n",
    "    z = random.random()\n",
    "    cum_prob = 0.0\n",
    "    for i in range(len(probs)):\n",
    "        prob = probs[i]\n",
    "        cum_prob += prob\n",
    "        if cum_prob > z:\n",
    "            return i\n",
    "  \n",
    "    return len(probs) - 1\n",
    "\n",
    "class Hedge:\n",
    "    def __init__(self, temperature, counts, values):\n",
    "        self.temperature = temperature\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        return\n",
    "  \n",
    "    def initialize(self, n_arms):\n",
    "        self.counts = [0 for col in range(n_arms)]\n",
    "        self.values = [0.0 for col in range(n_arms)]\n",
    "        return\n",
    "  \n",
    "    def select_arm(self):\n",
    "        z = sum([math.exp(v / self.temperature) for v in self.values])\n",
    "        probs = [math.exp(v / self.temperature) / z for v in self.values]\n",
    "        return categorical_draw(probs)\n",
    "  \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] = self.counts[chosen_arm] + 1\n",
    "    \n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = value + reward\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bc7f66b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:46:26.926122Z",
     "iopub.status.busy": "2022-08-09T02:46:26.925263Z",
     "iopub.status.idle": "2022-08-09T02:47:28.214418Z",
     "shell.execute_reply": "2022-08-09T02:47:28.213152Z"
    },
    "papermill": {
     "duration": 61.305766,
     "end_time": "2022-08-09T02:47:28.217155",
     "exception": false,
     "start_time": "2022-08-09T02:46:26.911389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best arm is 2\n"
     ]
    }
   ],
   "source": [
    "# hedge/test_hedge.py\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "means = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = list(map(lambda mu: BernoulliArm(mu), means))\n",
    "print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "f = open(\"D:\\coding\\bandit\\algorithms\\hedge\\hedge_results.tsv\", \"w\")\n",
    "\n",
    "for eta in [.5, .8, .9, 1, 2]:\n",
    "    algo = Hedge(eta, [], [])\n",
    "    algo.initialize(n_arms)\n",
    "    results = test_algorithm(algo, arms, 5000, 250)\n",
    "    for i in range(len(results[0])):\n",
    "        f.write(str(temperature) + \"\\t\")\n",
    "        f.write(\"\\t\".join([str(results[j][i]) for j in range(len(results))]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792a78b",
   "metadata": {
    "papermill": {
     "duration": 0.012674,
     "end_time": "2022-08-09T02:47:28.243206",
     "exception": false,
     "start_time": "2022-08-09T02:47:28.230532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20811b77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:47:28.271189Z",
     "iopub.status.busy": "2022-08-09T02:47:28.270483Z",
     "iopub.status.idle": "2022-08-09T02:47:28.279895Z",
     "shell.execute_reply": "2022-08-09T02:47:28.278992Z"
    },
    "papermill": {
     "duration": 0.026168,
     "end_time": "2022-08-09T02:47:28.282170",
     "exception": false,
     "start_time": "2022-08-09T02:47:28.256002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "arm1 = BernoulliArm(0.8)\n",
    "arm1.draw()\n",
    "arm1.draw()\n",
    "\n",
    "arm2 = BernoulliArm(0.7)\n",
    "arm2.draw()\n",
    "arm2.draw()\n",
    "\n",
    "arm3 = BernoulliArm(0.2)\n",
    "arm3.draw()\n",
    "arm3.draw()\n",
    "\n",
    "arms = [arm1, arm2, arm3]\n",
    "\n",
    "n_arms = len(arms)\n",
    "\n",
    "algo1 = EpsilonGreedy(0.1, [], [])\n",
    "algo2 = Softmax(1.0, [], [])\n",
    "algo3 = UCB1([], [])\n",
    "algo4 = Exp3(0.2, [])\n",
    "\n",
    "algos = [algo1, algo2, algo3, algo4]\n",
    "\n",
    "for algo in algos:\n",
    "    algo.initialize(n_arms)\n",
    "\n",
    "for t in range(10):\n",
    "    for algo in algos:\n",
    "        chosen_arm = algo.select_arm() # t시점에서 algo 번째 알고리즘에서 선택된 arm의 index\n",
    "         # 예를 들어 입실론 알고리즘이면, 입실론 만큼은 random한 index주고, 아니면 가장 큰 value가지고 있는 index값\n",
    "        reward = arms[chosen_arm].draw() # 돌아온 index값에 대해서 그 arm당겼을 때 얻은 보상\n",
    "         # 예를 들어 Bernoulli Arm이라면, 확률 p만큼은 1아니면 0 return   \n",
    "        algo.update(chosen_arm, reward) # 선택된 arm의 index값과 보상으로 각 알고리즘 업데이트\n",
    "         # 예를 들어 입실론 알고리즘에서 index = 0, 보상 1이면 그 값가지고 arm댕긴 횟수 counts올려주고,\n",
    "\n",
    "# print(algo1.counts)\n",
    "# print(algo1.values)\n",
    "\n",
    "# print(algo2.counts)\n",
    "# print(algo2.values)\n",
    "\n",
    "# print(algo3.counts)\n",
    "# print(algo3.values)\n",
    "\n",
    "# print(algo4.weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b999bd95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:47:28.310499Z",
     "iopub.status.busy": "2022-08-09T02:47:28.309803Z",
     "iopub.status.idle": "2022-08-09T02:47:28.857012Z",
     "shell.execute_reply": "2022-08-09T02:47:28.855905Z"
    },
    "papermill": {
     "duration": 0.564398,
     "end_time": "2022-08-09T02:47:28.859579",
     "exception": false,
     "start_time": "2022-08-09T02:47:28.295181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chosen_arms_algo1\n",
       "0.0    0.802294\n",
       "1.0    0.712036\n",
       "2.0    0.163683\n",
       "Name: reward_algo1, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sims = 1000\n",
    "horizon = 10\n",
    "results = test_algorithm(algo1, arms, num_sims, horizon)\n",
    "col_names = ['sim_nums_algo1', 'times_algo1','chosen_arms_algo1', 'reward_algo1', 'cumulative_rewards_algo1']\n",
    "results_temp = pd.DataFrame(results)\n",
    "results_algo1 = results_temp.T\n",
    "results_algo1.columns = col_names\n",
    "results_algo1.groupby(['chosen_arms_algo1'])['reward_algo1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f1a814d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:47:28.888357Z",
     "iopub.status.busy": "2022-08-09T02:47:28.887965Z",
     "iopub.status.idle": "2022-08-09T02:47:28.921563Z",
     "shell.execute_reply": "2022-08-09T02:47:28.920740Z"
    },
    "papermill": {
     "duration": 0.050667,
     "end_time": "2022-08-09T02:47:28.923613",
     "exception": false,
     "start_time": "2022-08-09T02:47:28.872946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>reward_algo1</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chosen_arms_algo1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1724</td>\n",
       "      <td>6996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>256</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>327</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "reward_algo1        0.0   1.0\n",
       "chosen_arms_algo1            \n",
       "0.0                1724  6996\n",
       "1.0                 256   633\n",
       "2.0                 327    64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(results_algo1.chosen_arms_algo1,results_algo1.reward_algo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f422a6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:47:28.952298Z",
     "iopub.status.busy": "2022-08-09T02:47:28.951249Z",
     "iopub.status.idle": "2022-08-09T02:47:28.977109Z",
     "shell.execute_reply": "2022-08-09T02:47:28.975944Z"
    },
    "papermill": {
     "duration": 0.042785,
     "end_time": "2022-08-09T02:47:28.979676",
     "exception": false,
     "start_time": "2022-08-09T02:47:28.936891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cumulative_rewards_algo1</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>10.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chosen_arms_algo1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>241</td>\n",
       "      <td>1175</td>\n",
       "      <td>1148</td>\n",
       "      <td>1158</td>\n",
       "      <td>1077</td>\n",
       "      <td>1082</td>\n",
       "      <td>1029</td>\n",
       "      <td>841</td>\n",
       "      <td>584</td>\n",
       "      <td>291</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>14</td>\n",
       "      <td>62</td>\n",
       "      <td>89</td>\n",
       "      <td>121</td>\n",
       "      <td>128</td>\n",
       "      <td>142</td>\n",
       "      <td>122</td>\n",
       "      <td>102</td>\n",
       "      <td>75</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>55</td>\n",
       "      <td>51</td>\n",
       "      <td>43</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "cumulative_rewards_algo1  0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0   \\\n",
       "chosen_arms_algo1                                                          \n",
       "0.0                        241  1175  1148  1158  1077  1082  1029   841   \n",
       "1.0                         14    62    89   121   128   142   122   102   \n",
       "2.0                         32    53    47    55    51    43    53    31   \n",
       "\n",
       "cumulative_rewards_algo1  8.0   9.0   10.0  \n",
       "chosen_arms_algo1                           \n",
       "0.0                        584   291    94  \n",
       "1.0                         75    31     3  \n",
       "2.0                         22     3     1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo1_cumulative = pd.crosstab(results_algo1.chosen_arms_algo1,results_algo1.cumulative_rewards_algo1)\n",
    "algo1_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf39ad00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:47:29.008804Z",
     "iopub.status.busy": "2022-08-09T02:47:29.007797Z",
     "iopub.status.idle": "2022-08-09T02:47:29.017184Z",
     "shell.execute_reply": "2022-08-09T02:47:29.016217Z"
    },
    "papermill": {
     "duration": 0.026074,
     "end_time": "2022-08-09T02:47:29.019274",
     "exception": false,
     "start_time": "2022-08-09T02:47:28.993200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1       2.0\n",
       "2       2.0\n",
       "3       3.0\n",
       "4       4.0\n",
       "       ... \n",
       "9995    4.0\n",
       "9996    5.0\n",
       "9997    5.0\n",
       "9998    5.0\n",
       "9999    6.0\n",
       "Name: cumulative_rewards_algo1, Length: 10000, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_algo1.cumulative_rewards_algo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb0a6ce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:47:29.049442Z",
     "iopub.status.busy": "2022-08-09T02:47:29.048341Z",
     "iopub.status.idle": "2022-08-09T02:47:29.053080Z",
     "shell.execute_reply": "2022-08-09T02:47:29.052215Z"
    },
    "papermill": {
     "duration": 0.021987,
     "end_time": "2022-08-09T02:47:29.055288",
     "exception": false,
     "start_time": "2022-08-09T02:47:29.033301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# sns.barplot('cumulative_rewards_algo1','chosen_arms_algo1',data=results_algo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ce35d22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:47:29.084249Z",
     "iopub.status.busy": "2022-08-09T02:47:29.083796Z",
     "iopub.status.idle": "2022-08-09T02:47:29.088073Z",
     "shell.execute_reply": "2022-08-09T02:47:29.087186Z"
    },
    "papermill": {
     "duration": 0.021481,
     "end_time": "2022-08-09T02:47:29.090263",
     "exception": false,
     "start_time": "2022-08-09T02:47:29.068782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sns.distplot(results_algo1[results_algo1['chosen_arms_algo1']==0].cumulative_rewards_algo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51086fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:47:29.119303Z",
     "iopub.status.busy": "2022-08-09T02:47:29.118867Z",
     "iopub.status.idle": "2022-08-09T02:47:29.461063Z",
     "shell.execute_reply": "2022-08-09T02:47:29.460292Z"
    },
    "papermill": {
     "duration": 0.359618,
     "end_time": "2022-08-09T02:47:29.463478",
     "exception": false,
     "start_time": "2022-08-09T02:47:29.103860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chosen_arms_algo2\n",
       "0.0    0.796709\n",
       "1.0    0.702798\n",
       "2.0    0.212074\n",
       "Name: reward_algo2, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sims = 1000\n",
    "horizon = 10\n",
    "results = test_algorithm(algo2, arms, num_sims, horizon)\n",
    "col_names = ['sim_nums_algo2', 'times_algo2','chosen_arms_algo2', 'reward_algo2', 'cumulative_rewards_algo2']\n",
    "results_temp = pd.DataFrame(results)\n",
    "results_algo2 = results_temp.T\n",
    "results_algo2.columns = col_names\n",
    "results_algo2.groupby(['chosen_arms_algo2'])['reward_algo2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cafdefd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:47:29.493079Z",
     "iopub.status.busy": "2022-08-09T02:47:29.492330Z",
     "iopub.status.idle": "2022-08-09T02:47:29.830204Z",
     "shell.execute_reply": "2022-08-09T02:47:29.829281Z"
    },
    "papermill": {
     "duration": 0.355247,
     "end_time": "2022-08-09T02:47:29.832776",
     "exception": false,
     "start_time": "2022-08-09T02:47:29.477529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chosen_arms_algo3\n",
       "0.0    0.799423\n",
       "1.0    0.699449\n",
       "2.0    0.193928\n",
       "Name: reward_algo3, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sims = 1000\n",
    "horizon = 10\n",
    "results = test_algorithm(algo3, arms, num_sims, horizon)\n",
    "col_names = ['sim_nums_algo3', 'times_algo3','chosen_arms_algo3', 'reward_algo3', 'cumulative_rewards_algo3']\n",
    "results_temp = pd.DataFrame(results)\n",
    "results_algo3 = results_temp.T\n",
    "results_algo3.columns = col_names\n",
    "results_algo3.groupby(['chosen_arms_algo3'])['reward_algo3'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 289.703029,
   "end_time": "2022-08-09T02:47:32.938854",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-09T02:42:43.235825",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
